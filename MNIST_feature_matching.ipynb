{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST WGAN with feature matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100000\n",
    "h_dim = 128\n",
    "z_dim = 10\n",
    "image_dim = mnist.train.images.shape[1]\n",
    "target_dim = mnist.train.labels.shape[1]\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim, image_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.G = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.G(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, h_dim, image_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.D = nn.Sequential(\n",
    "            nn.Linear(image_dim, h_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.out = nn.Linear(h_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat = self.D(x)\n",
    "        out = self.out(feat)\n",
    "        return feat, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom JSD Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSDLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JSDLoss,self).__init__()\n",
    "\n",
    "    def forward(self, f_real, f_synt):\n",
    "        assert f_real.size()[1] == f_synt.size()[1]\n",
    "\n",
    "        f_num_features = f_real.size()[1]\n",
    "        batch_size = f_real.size()[0]\n",
    "        identity = autograd.Variable(torch.eye(f_num_features)*0.1)\n",
    "\n",
    "        if use_cuda:\n",
    "            identity = identity.cuda(gpu)\n",
    "\n",
    "        f_real_mean = torch.mean(f_real, 0, keepdim=True)\n",
    "        f_synt_mean = torch.mean(f_synt, 0, keepdim=True)\n",
    "\n",
    "        dev_f_real = f_real - f_real_mean.expand(batch_size,f_num_features) # batch_size x num_feat\n",
    "        dev_f_synt = f_synt - f_synt_mean.expand(batch_size,f_num_features) # batch_size x num_feat\n",
    "\n",
    "        f_real_xx = torch.mm(torch.t(dev_f_real), dev_f_real) # num_feat x batch_size * batch_size x num_feat = num_feat x num_feat\n",
    "        f_synt_xx = torch.mm(torch.t(dev_f_synt), dev_f_synt) # num_feat x batch_size * batch_size x num_feat = num_feat x num_feat\n",
    "\n",
    "        cov_mat_f_real = f_real_xx / (batch_size-1) - torch.mm(f_real_mean, torch.t(f_real_mean)) + identity # num_feat x num_feat\n",
    "        cov_mat_f_synt = f_synt_xx / (batch_size-1) - torch.mm(f_synt_mean, torch.t(f_synt_mean)) + identity # num_feat x num_feat\n",
    "\n",
    "        cov_mat_f_real_inv = torch.inverse(cov_mat_f_real)\n",
    "        cov_mat_f_synt_inv = torch.inverse(cov_mat_f_synt)\n",
    "\n",
    "#         temp1 = torch.trace(torch.add(torch.mm(cov_mat_f_synt_inv, torch.t(cov_mat_f_real)), torch.mm(cov_mat_f_real_inv, torch.t(cov_mat_f_synt))))\n",
    "        temp1 = torch.trace(torch.add(torch.mm(cov_mat_f_synt_inv, cov_mat_f_real), torch.mm(cov_mat_f_real_inv, cov_mat_f_synt)))\n",
    "#         temp1 = temp1.view(1,1)\n",
    "        temp2 = torch.mm(torch.mm((f_synt_mean - f_real_mean), (cov_mat_f_synt_inv + cov_mat_f_real_inv)), torch.t(f_synt_mean - f_real_mean))\n",
    "        loss_g = temp1 + temp2\n",
    "\n",
    "        return loss_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_grad():\n",
    "    G.zero_grad()\n",
    "    D.zero_grad()\n",
    "\n",
    "G = Generator(z_dim, h_dim, image_dim)\n",
    "D = Discriminator(h_dim, image_dim)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=learning_rate)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=learning_rate)\n",
    "jsdloss = JSDLoss()\n",
    "\n",
    "if use_cuda:\n",
    "    jsdloss = jsdloss.cuda(gpu)\n",
    "    D = D.cuda(gpu)\n",
    "    G = G.cuda(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; D_loss: [-0.00508218]; G_loss: [[257.86005]]\n",
      "Iter-100; D_loss: [-0.0068009]; G_loss: [[257.62244]]\n",
      "Iter-200; D_loss: [-0.00802178]; G_loss: [[257.56207]]\n",
      "Iter-300; D_loss: [-0.00741378]; G_loss: [[257.5558]]\n",
      "Iter-400; D_loss: [-0.00689281]; G_loss: [[257.23676]]\n",
      "Iter-500; D_loss: [-0.01160111]; G_loss: [[259.22345]]\n",
      "Iter-600; D_loss: [-0.00850669]; G_loss: [[257.55383]]\n",
      "Iter-700; D_loss: [-0.00958304]; G_loss: [[257.7016]]\n",
      "Iter-800; D_loss: [-0.01513332]; G_loss: [[259.2674]]\n",
      "Iter-900; D_loss: [-0.01600967]; G_loss: [[259.10782]]\n",
      "Iter-1000; D_loss: [-0.01149559]; G_loss: [[258.29175]]\n",
      "Iter-1100; D_loss: [-0.0139805]; G_loss: [[258.54272]]\n",
      "Iter-1200; D_loss: [-0.01193056]; G_loss: [[257.58734]]\n",
      "Iter-1300; D_loss: [-0.01052928]; G_loss: [[257.3488]]\n",
      "Iter-1400; D_loss: [-0.01216921]; G_loss: [[257.9445]]\n",
      "Iter-1500; D_loss: [-0.00849424]; G_loss: [[257.4827]]\n",
      "Iter-1600; D_loss: [-0.00964132]; G_loss: [[256.9805]]\n",
      "Iter-1700; D_loss: [-0.01641277]; G_loss: [[259.39554]]\n",
      "Iter-1800; D_loss: [-0.01366457]; G_loss: [[258.24445]]\n",
      "Iter-1900; D_loss: [-0.01804233]; G_loss: [[260.70712]]\n",
      "Iter-2000; D_loss: [-0.01016082]; G_loss: [[257.4081]]\n",
      "Iter-2100; D_loss: [-0.01252866]; G_loss: [[257.62228]]\n",
      "Iter-2200; D_loss: [-0.01156851]; G_loss: [[257.8068]]\n",
      "Iter-2300; D_loss: [-0.01256509]; G_loss: [[257.26477]]\n",
      "Iter-2400; D_loss: [-0.0106255]; G_loss: [[257.78223]]\n",
      "Iter-2500; D_loss: [-0.00334029]; G_loss: [[256.9581]]\n",
      "Iter-2600; D_loss: [-0.00814151]; G_loss: [[256.96396]]\n",
      "Iter-2700; D_loss: [-0.01138785]; G_loss: [[257.1497]]\n",
      "Iter-2800; D_loss: [-0.00385426]; G_loss: [[256.58115]]\n",
      "Iter-2900; D_loss: [-0.01055336]; G_loss: [[258.20615]]\n",
      "Iter-3000; D_loss: [-0.00778122]; G_loss: [[256.84546]]\n",
      "Iter-3100; D_loss: [-0.00926711]; G_loss: [[257.45883]]\n",
      "Iter-3200; D_loss: [-0.00856493]; G_loss: [[257.0313]]\n",
      "Iter-3300; D_loss: [-0.00733258]; G_loss: [[257.02228]]\n",
      "Iter-3400; D_loss: [-0.00931399]; G_loss: [[257.7847]]\n",
      "Iter-3500; D_loss: [-0.00752013]; G_loss: [[256.77478]]\n",
      "Iter-3600; D_loss: [-0.01047809]; G_loss: [[256.61108]]\n",
      "Iter-3700; D_loss: [-0.00593867]; G_loss: [[257.12518]]\n",
      "Iter-3800; D_loss: [-0.00840829]; G_loss: [[257.03067]]\n",
      "Iter-3900; D_loss: [-0.00846784]; G_loss: [[257.13275]]\n",
      "Iter-4000; D_loss: [-0.00907041]; G_loss: [[257.16245]]\n",
      "Iter-4100; D_loss: [-0.00567665]; G_loss: [[256.54706]]\n",
      "Iter-4200; D_loss: [-0.01188204]; G_loss: [[257.17053]]\n",
      "Iter-4300; D_loss: [-0.00865046]; G_loss: [[256.7022]]\n",
      "Iter-4400; D_loss: [-0.00345929]; G_loss: [[256.29025]]\n",
      "Iter-4500; D_loss: [-0.00531248]; G_loss: [[256.51334]]\n",
      "Iter-4600; D_loss: [-0.00766614]; G_loss: [[256.7306]]\n",
      "Iter-4700; D_loss: [-0.00664994]; G_loss: [[256.677]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-e101f7b37d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mG_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsdloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_real_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_fake_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mG_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mG_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kaggle/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kaggle/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for _ in range(5):\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = True\n",
    "        \n",
    "        G = G.eval()\n",
    "        D = D.train()\n",
    "        D.zero_grad()\n",
    "        z = autograd.Variable(torch.randn(batch_size, z_dim))\n",
    "        X, _ = mnist.train.next_batch(batch_size)\n",
    "        X = autograd.Variable(torch.from_numpy(X))\n",
    "\n",
    "        G_sample = G(z)\n",
    "        D_real_feat, D_real_out = D(X)\n",
    "        D_fake_feat, D_fake_out = D(G_sample)\n",
    "\n",
    "        D_loss_GAN = -(torch.mean(D_real_out) - torch.mean(D_fake_out))\n",
    "        #D_loss_feat_matching = jsdloss(D_real_feat, D_fake_feat)\n",
    "        \n",
    "        D_loss = D_loss_GAN# + D_loss_feat_matching\n",
    "\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        D_optimizer.step()\n",
    "\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "        reset_grad()\n",
    "    \n",
    "    for p in D.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    G = G.train()\n",
    "    D = D.eval()\n",
    "    G.zero_grad()\n",
    "\n",
    "    X, _ = mnist.train.next_batch(batch_size)\n",
    "    X = autograd.Variable(torch.from_numpy(X))\n",
    "    z = autograd.Variable(torch.randn(batch_size, z_dim))\n",
    "\n",
    "    G_sample = G(z)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    G_loss = jsdloss(D_real_feat, D_fake_feat)\n",
    "\n",
    "    G_loss.backward(retain_graph=True)\n",
    "    G_optimizer.step()\n",
    "\n",
    "    reset_grad()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'\n",
    "              .format(epoch, D_loss.data.numpy(), G_loss.data.numpy()))\n",
    "        samples = G(z).data.numpy()[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        epoch += 1\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
